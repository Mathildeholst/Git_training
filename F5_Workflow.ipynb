{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbJzFDb6pQitMfs9zY3OyP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mathildeholst/Git_training/blob/main/F5_Workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning your model using KerasTuner\n"
      ],
      "metadata": {
        "id": "QDQaH2DH5sKG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqPA7ugg5ql5",
        "outputId": "4af5ec15-a966-4081-8070-9e3ee4a8037f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m112.6/129.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install keras-tuner -q\n",
        "import keras_tuner as kt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "def build_model(hp):\n",
        "    units = hp.Int(name=\"units\", min_value=16, max_value=64, step=16)\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(units, activation=\"relu\"),\n",
        "        layers.Dense(10, activation=\"softmax\")\n",
        "    ])\n",
        "    optimizer = hp.Choice(name=\"optimizer\", values=[\"rmsprop\", \"adam\"])\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What does this KerasTuner code do?\n",
        "* You're building a model where you don't hardcode the hyperparameters.\n",
        "\n",
        "Instead, you let KerasTuner test different options automatically.\n",
        "\n",
        "* hp.Int: tries different values for units (16, 32, 48, 64)\n",
        "\n",
        "* hp.Choice: lets KerasTuner pick an optimizer (either \"rmsprop\" or \"adam\")\n",
        "\n",
        "\n",
        "You can also use:\n",
        "\n",
        "* hp.Float: try decimal values (e.g., learning rate)\n",
        "\n",
        "* hp.Boolean: choose between True or False\n",
        "\n",
        "This helps you find the best model configuration without guessing everything yourself."
      ],
      "metadata": {
        "id": "V_wgV2L16FMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = kt.BayesianOptimization(\n",
        "    build_model,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_trials=100,\n",
        "    executions_per_trial=2,\n",
        "    directory=\"mnist_kt_test\",\n",
        "    overwrite=True,\n",
        ")"
      ],
      "metadata": {
        "id": "cPoN44yh7LXc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "objective=\"val_accuracy\",\n",
        "* Specify the metric that the tuner will seek to optimize. Always specify validation metrics, since the goal of the search process is to find models that generalize!\n",
        "\n",
        "max_trials=100,\n",
        "* Maximum number of different model configurations (“trials”) to try before ending the search.\n",
        "\n",
        "executions_per_trial=2,\n",
        " * executions_per_trial trains the same model setup multiple times and averages the results, so you get more reliable scores.\n",
        "\n",
        "directory=\"mnist_kt_test\",\n",
        "* Where to store search logs\n",
        "\n",
        "overwrite=True,\n",
        "* overwrite=True means KerasTuner will delete old search results and start fresh. Use this if you’ve changed your model setup. Set it to False if you want to continue tuning from where you left off.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QmMyYOFb7b_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5),\n",
        "]\n",
        "tuner.search(\n",
        "    x_train, y_train,\n",
        "    batch_size=128,\n",
        "    epochs=100,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=callbacks,\n",
        "    verbose=2,\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "08th_wA-8j-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses KerasTuner to find the best model setup.\n",
        "\n",
        "You train for up to 100 epochs, but:\n",
        "\n",
        "* You don’t know how many epochs are actually needed.\n",
        "\n",
        "* So you use EarlyStopping: it watches the validation loss (val_loss) and stops training if it doesn't improve for 5 rounds (patience=5).\n",
        "\n",
        "This helps avoid overfitting and saves time.\n",
        "\n",
        "verbose=2\n",
        "* verbose=2 is a clean and readable way to follow training progress\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HZa5ch0M9Ms6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 4\n",
        "best_hps = tuner.get_best_hyperparameters(top_n)"
      ],
      "metadata": {
        "id": "FyL41iju-Wr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This gives you the top 4 best hyperparameter sets found during tuning.\n",
        "* You can then use one of them to build and train your final model."
      ],
      "metadata": {
        "id": "ccZ4yaqF-XTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_best_epoch(hp):\n",
        "    model = build_model(hp)\n",
        "    callbacks=[\n",
        "        keras.callbacks.EarlyStopping(\n",
        "            monitor=\"val_loss\", mode=\"min\", patience=10)\n",
        "    ]\n",
        "    history = model.fit(\n",
        "        x_train, y_train,\n",
        "        validation_data=(x_val, y_val),\n",
        "        epochs=100,\n",
        "        batch_size=128,\n",
        "        callbacks=callbacks)\n",
        "    val_loss_per_epoch = history.history[\"val_loss\"]\n",
        "    best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1 # Python uses zero-based indexing, so we add 1 to get human-readable epoch number\n",
        "    print(f\"Best epoch: {best_epoch}\")\n",
        "    return best_epoch"
      ],
      "metadata": {
        "id": "cNLCEr3V_XEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function helps you find the best number of epochs to train your model, using early stopping.\n",
        "\n",
        "Trains it for up to 100 epochs, but stops early if validation loss doesn't improve for 10 epochs.\n",
        "\n",
        "You want to know how many epochs to train your final model without overfitting. This helps you train just enough — not too much, not too little."
      ],
      "metadata": {
        "id": "4_6K3zsz_mom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_best_trained_model(hp):\n",
        "    best_epoch = get_best_epoch(hp)\n",
        "    model = build_model(hp)\n",
        "    model.fit(\n",
        "        x_train_full, y_train_full,\n",
        "        batch_size=128,\n",
        "        epochs=int(best_epoch * 1.2))\n",
        "    return model"
      ],
      "metadata": {
        "id": "pkIO8x3v_0MA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function trains the final model using the best number of epochs found earlier.\n",
        "\n",
        "Trains it on all the training data (x_train_full, y_train_full) — including what was previously used for validation.\n",
        "\n",
        "Trains for a bit longer than before (about 20% more epochs) to make the most of the full dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "EKfLfaR7_07X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_models = []\n",
        "for hp in best_hps:\n",
        "    model = get_best_trained_model(hp)\n",
        "    model.evaluate(x_test, y_test)\n",
        "    best_models.append(model)"
      ],
      "metadata": {
        "id": "Xwnmv7krAcxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code trains and evaluates the best models found by KerasTuner."
      ],
      "metadata": {
        "id": "Y2hpTW2pAjB8"
      }
    }
  ]
}