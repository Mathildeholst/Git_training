{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mathildeholst/Git_training/blob/main/chapter02_mathematical-building-blocks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5sj3pRijPBq"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8r9Fq6ojPBr"
      },
      "source": [
        "# The mathematical building blocks of neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GIT079bjPBs"
      },
      "source": [
        "## A first look at a neural network - F1\n",
        "\n",
        "*   Listepunkt\n",
        "*   Listepunkt\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCCB4YXMjPBs"
      },
      "source": [
        "**Loading the MNIST dataset in Keras**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F--rBTBdjPBs",
        "outputId": "ad4086c7-5d39-4005-b564-576704f014ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MNIST is a famous dataset used for training machine learning models to recognize handwritten digits (0-9).\n",
        "Multi-class: There are 10 possible classes (digits 0 to 9).\n",
        "Classification task: The goal is to predict the correct digit for each image.\n",
        "*   Training set: 60,000 labeled images used to teach the Model\n",
        "*   Test set: 10,000 labeled images used to check how well the model learned."
      ],
      "metadata": {
        "id": "tuOFlUNUkJVG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqQom-3fjPBs",
        "outputId": "cb4dcfa0-5571-45d8-c8d0-10dadc6c2e30"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "train_images.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   60,000 → Number of images in the training set.\n",
        "*   28 × 28 → Each image is 28x28 pixels (grayscale, no color channels).\n",
        "\n"
      ],
      "metadata": {
        "id": "ios-LfqflIWq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOCnY_TjjPBt",
        "outputId": "7e2bde9d-d157-4e1f-b06c-005a589a0cb1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "len(train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "len(train_labels) = 60,000 means there are 60,000 labels, one for each training image."
      ],
      "metadata": {
        "id": "5A_OeI2KlqOu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kReBUdXgjPBt",
        "outputId": "2082353d-c9b2-4b2b-b98a-e2922d744090"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "train_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "train_labels is an array of numbers (0-9), where each number represents the correct digit for a corresponding image in train_images.\n",
        "\n",
        "* The first image is a 5\n",
        "* The second image is a 0\n",
        "* The third image is a 4\n",
        "\n",
        "These labels are used to train the model by telling it the correct answer for each image.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cWMzEckhl6f5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqQ8B063jPBt",
        "outputId": "db4b7966-e373-4370-841e-55fc4b479fb4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "test_images.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 10,000 → Number of images in the test set.\n",
        "* 28 × 28 → Each image is 28x28 pixels (grayscale, no color channels)."
      ],
      "metadata": {
        "id": "0IIB2G1Ym033"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TFHiLoGjPBt",
        "outputId": "bfa4ef9a-2b05-4ce2-f048-fd2918c58e6d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "len(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CtPp-YMjPBt",
        "outputId": "b502541f-2f5e-4cb2-f8a8-61d6fa6baef0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "test_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* train_images = The input (handwritten digit pictures).\n",
        "* train_labels = The output (correct number for each image).\n",
        "* test_images → Images of handwritten digits (10,000 images, each 28x28 pixels).\n",
        "* test_labels → The correct answers (10,000 labels, each a digit from 0-9).\n"
      ],
      "metadata": {
        "id": "p59J9HSvpG4m"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrWMUCWcjPBt"
      },
      "source": [
        "**The network architecture**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmv7Un6ijPBt"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(512, activation=\"relu\"),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*1️⃣ Importing Keras and Layers*\n",
        "* keras: A deep learning library inside TensorFlow.\n",
        "* layers: Used to build layers for the neural network.\n",
        "\n",
        "*2️⃣ Creating a Sequential Model*\n",
        "* keras.Sequential([]) → A simple way to stack layers one after another in order.\n",
        "* A sequential fully connected (dense) network is a neural network where every neuron in one layer is connected to every neuron in the next layer.\n",
        "\n",
        "*3️⃣ First Layer (Hidden Layer)*\n",
        "* Dense(512) → A fully connected hidden layer with 512 neurons.\n",
        "  * 512 is an arbitrary choice but commonly used for deep learning models. A higher number of neurons helps the model learn complex patterns in the input data.\n",
        "* activation=\"relu\" → Uses ReLU (Rectified Linear Unit), which helps the model learn complex patterns efficiently.\n",
        "\n",
        "*4️⃣ Second Layer (Output Layer)*\n",
        "* Dense(10) → The output layer with 10 neurons (one for each digit 0-9).\n",
        "  *  There are 10 possible digits (0-9), so we need 10 neurons—one for each class.\n",
        "* activation=\"softmax\" → Converts the outputs into probabilities, so the model can pick the most likely digit."
      ],
      "metadata": {
        "id": "i-h2DOUiqYsY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dEIAOOmjPBu"
      },
      "source": [
        "**The compilation step**\n",
        "\n",
        "Get the network ready for training (compilation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-Y7Fu6VjPBu"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* An optimizer — how the model will update itself when it sees training data\n",
        "* A loss function - how the model will measure its performance\n",
        "* metrics to monitor - here, accuracy\n",
        "\n",
        "* optimizer=\"rmsprop\" → Helps adjust weights efficiently for better learning.\n",
        "* loss=\"sparse_categorical_crossentropy\" → Used for multi-class classification (digits 0-9).\n",
        "* metrics=[\"accuracy\"] → Tracks how often predictions are correct.\n"
      ],
      "metadata": {
        "id": "EcHkdwH_vhbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_images.min(), train_images.max())"
      ],
      "metadata": {
        "id": "n-i8c9AK2NFf",
        "outputId": "52aa45ac-a136-4b98-b8a5-9688fcb24333",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shows the range of the pixels value used below"
      ],
      "metadata": {
        "id": "Td4SGXNE2OyJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYRXqocsjPBu"
      },
      "source": [
        "**Preparing the image data**\n",
        "\n",
        "Transform images to a float32 array of shape (60000, 28 * 28) with values between 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctesalWtjPBu"
      },
      "outputs": [],
      "source": [
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype(\"float32\") / 255"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*1. Reshaping the Images*\n",
        "\n",
        "* The original images are 28x28 pixels.\n",
        "* reshape((60000, 28 * 28)) converts them into a flat 1D array of 784 pixels per image.\n",
        "\n",
        "*2. Normalizing the Pixel Values*\n",
        "* The pixel values originally range from 0 to 255.\n",
        "* Dividing by 255 scales them to a range between 0 and 1.\n",
        "* In grayscale images (like MNIST), each pixel is represented by an integer from 0 to 255.\n",
        "\n"
      ],
      "metadata": {
        "id": "lWxPrGR5wc6K"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKS4xV_hjPBu"
      },
      "source": [
        "**\"Fitting\" the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl48kINNjPBu",
        "outputId": "99f97935-1732-4bf1-8b89-4a6905255e40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.8759 - loss: 0.4314\n",
            "Epoch 2/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.9656 - loss: 0.1165\n",
            "Epoch 3/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9796 - loss: 0.0708\n",
            "Epoch 4/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9848 - loss: 0.0490\n",
            "Epoch 5/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9893 - loss: 0.0364\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c1373dffcd0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "model.fit(train_images, train_labels, epochs=5, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* train_images and train_labels → The training data (images and correct labels).\n",
        "* epochs=5 → The model goes through the entire dataset 5 times to improve learning.\n",
        "* batch_size=128 → The model processes 128 images at a time before updating weights.\n",
        "\n",
        "*Why Use Batches?*\n",
        "* Training on all images at once is too slow.\n",
        "* Using smaller batches (128 at a time) makes training faster and more efficient.\n",
        "* Small batch sizes (e.g., 16, 32) More updates, but training is slower because the model updates too often.\n",
        "* Large batch sizes (e.g., 256, 512, 1024)\n",
        "Faster, but may not generalize well (can get stuck in local minima).\n",
        "\n",
        "SE UDD. I FORELSÆNING 1 - Train the model\n"
      ],
      "metadata": {
        "id": "AT3h3FXj2soy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm5WRE5ejPBu"
      },
      "source": [
        "**Using the model to make predictions**\n",
        "\n",
        "Take the 10 first images in the test set and predict for these\n",
        "\n",
        "Then look at the probabilities for the first image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezPl1y6OjPBu",
        "outputId": "43fad66d-cc76-47ed-e42f-9e6219149e54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3.9813060e-07, 2.0947910e-08, 4.9943937e-06, 1.9326540e-04,\n",
              "       1.4754845e-11, 4.9837286e-07, 4.6406160e-11, 9.9978423e-01,\n",
              "       7.6238928e-07, 1.5947255e-05], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "test_digits = test_images[0:10]\n",
        "predictions = model.predict(test_digits)\n",
        "predictions[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* test_digits = test_images[0:10] = Extracts the first 10 images from test_images.\n",
        "* predictions = model.predict(test_digits) =The model processes the 10 test images and predicts their digit labels.\n",
        "* predictions[0] = Displays the prediction for the first test images\n",
        "  * The output is an array of 10 values, each representing the model's confidence in the image being a specific digit (0-9)."
      ],
      "metadata": {
        "id": "2k9N6l9c5mai"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_4jrVvljPBu",
        "outputId": "53ef83da-339f-427d-cb36-73d4b54a7c10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "predictions[0].argmax()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* predictions[0] → This is an array of 10 probability values, one for each digit (0-9).\n",
        "* .argmax() → Returns the index of the highest probability, which is the model’s predicted digit.\n",
        "\n",
        "*it means the model predicts that the first test image is a \"7\".*"
      ],
      "metadata": {
        "id": "p_WboJXi6Ugo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3yy_ip7jPBu",
        "outputId": "28747af2-d6be-4576-e270-3a81c346a390",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.99978423"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "predictions[0][7]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The value 0.99978423 means the model is 99.98% confident that the first test image is a 7.\n",
        "* This is a very high probability, meaning the model is almost certain that the image is a 7."
      ],
      "metadata": {
        "id": "-uDyBgAW7QbG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Py5s93QijPBu",
        "outputId": "31f07373-8eba-4514-d845-eed9fc311920",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "test_labels[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since test_labels[0] returns 7, it means the actual number in the first test image is 7.\n",
        "* Man bruger test_labels, da det er de rigtige værdier, hvor i mod test images er de håndskrevet tal\n",
        "\n"
      ],
      "metadata": {
        "id": "NfEK2g777wtQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_HVNpZtjPBu"
      },
      "source": [
        "**Evaluating the model on new data**\n",
        "\n",
        "On average, how good is our model at classifying such never-before-seen digits?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2PPEC_1jPBu",
        "outputId": "fcf8bb65-ce6d-49e0-a7e4-caf277efffcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9761 - loss: 0.0732\n",
            "test_acc: 0.9810000061988831\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"test_acc: {test_acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Runs the trained model on the test dataset (test_images).\n",
        "* Compares the model's predictions with the actual labels (test_labels).\n",
        "* test_acc: 0.9810000061988831 → The final test accuracy is 98.1%.\n",
        "  * This means the model correctly classified 98.1% of the test images.\n",
        "  * loss: 0.0732 → The loss value, which measures how far predictions were from the correct answers. Lower is better."
      ],
      "metadata": {
        "id": "lC9bNrGY8Kjy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpHX6tbSjPBu"
      },
      "source": [
        "## Data representations for neural networks - F2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CSaf4mIjPBu"
      },
      "source": [
        "### Scalars (rank-0 tensors)\n",
        "A tensor with only 1 number is a scalar. Here is a NumPy scalar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z2r20gEGjPBu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e5105d4-84fc-471d-c6af-f69f1828f0eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(12)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "x = np.array(12)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Y2jUcCqcjPBu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09f8a5c6-6bcd-4be2-baf3-4f2978651b3c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "x.ndim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import numpy as np\n",
        "* This imports the NumPy library, which is used for handling arrays and numerical data.\n",
        "\n",
        "x = np.array(12)\n",
        "* This creates a NumPy array with a single number, 12.\n",
        "* So x is a scalar (just one value), stored as a 0-dimensional array (also called a rank 0 tensor). This is also called a 0-D tensor in deep learning.\n",
        "\n",
        "x.ndim\n",
        "* ndim stands for \"number of dimensions\"\n",
        "* Since x is just a single number, it has 0 dimensions"
      ],
      "metadata": {
        "id": "Sbpw2ZiXkeIy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgb94iTsjPBu"
      },
      "source": [
        "### Vectors (rank-1 tensors)\n",
        "A tensor with an array of numbers is a vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LG0Uduk8jPBu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75b4f298-9e52-48f1-ed08-63edb3da9101"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([12,  3,  6, 14,  7])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "x = np.array([12, 3, 6, 14, 7])\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DYYUTQgejPBu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a87a7579-d062-41fd-de57-bc4b1ef0c63c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "x.ndim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a 5-dimensional vector, but it’s a 1-dimensional tensor (a tensor of rank 1)!\n",
        "\n",
        "vector = rank-1 tensor = 1D tensor"
      ],
      "metadata": {
        "id": "Tt8Q7mX8lOjm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94WkEvHTjPBv"
      },
      "source": [
        "### Matrices (rank-2 tensors)\n",
        "A tensor with an array of vectors is a matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HL6HYv1zjPBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58e73dbf-f42b-41d8-8034-a270de97502a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "x = np.array([[5, 78, 2, 34, 0],\n",
        "              [6, 79, 3, 35, 1],\n",
        "              [7, 80, 4, 36, 2]])\n",
        "x.ndim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a matrix in math, or a 2-D tensor in deep learning.\n",
        "\n",
        "**Eks. **\n",
        "\n",
        "An actuarial dataset of people, where we consider each person’s age, gender, and income. Each person can be characterized as a vector of 3 values, and thus an entire dataset of 100,000 people can be stored in a rank-2 tensor of shape (100000, 3).\n"
      ],
      "metadata": {
        "id": "Gp0HCDN9l2gF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2TpnLQHjPBv"
      },
      "source": [
        "### Rank-3 and higher-rank tensors\n",
        "A tensor with an array of matrices is a rank-3 tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "49Gk9s2ZjPBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "376286d3-210e-4552-bab3-a28b26856a51"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "x = np.array([[[5, 78, 2, 34, 0],\n",
        "               [6, 79, 3, 35, 1],\n",
        "               [7, 80, 4, 36, 2]],\n",
        "              [[5, 78, 2, 34, 0],\n",
        "               [6, 79, 3, 35, 1],\n",
        "               [7, 80, 4, 36, 2]],\n",
        "              [[5, 78, 2, 34, 0],\n",
        "               [6, 79, 3, 35, 1],\n",
        "               [7, 80, 4, 36, 2]]])\n",
        "x.ndim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This array has 3 dimensions:\n",
        "\n",
        "* Depth (3 blocks)\n",
        "\n",
        "* Rows (3 per block)\n",
        "\n",
        "* Columns (5 per row)\n",
        "\n",
        "This is called a 3D tensor or a rank 3 tensor."
      ],
      "metadata": {
        "id": "JVglABZrnGRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZnBxMFqnBqs",
        "outputId": "7a819d45-f0d1-4ca6-dfe5-d292fccaf2d9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 3, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bekræfter ovenstående"
      ],
      "metadata": {
        "id": "HffanMotnqH2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCXJyIWcjPBv"
      },
      "source": [
        "### Key attributes\n",
        "Vælg billede og sådan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Qb15tRcUjPBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c3511a8-3459-497d-9017-1af9a9f50165"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loads the MNIST dataset (70,000 handwritten digit images)\n",
        "\n",
        "It gives you:\n",
        "\n",
        "* train_images → 60,000 images for training\n",
        "* train_labels → their correct digits (0–9)\n",
        "* test_images → 10,000 images for testing\n",
        "* test_labels → their correct digits"
      ],
      "metadata": {
        "id": "sSqJpZlXskbv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mGRt0yNEjPBy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cd79980-ec3f-415b-ad3e-72c8130faa3e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "train_images.ndim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4Mn0gvH5jPBy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "621dec86-dc83-44bc-d12c-887a6e279f1b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "train_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BI1H-vKwjPBy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a86fb1e-19f5-4dc1-ebb1-04dd115df7aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('uint8')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "train_images.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* train_images.ndim → Tells how many dimensions the data has (here: 3).\n",
        "* train_images.shape → Shows the size: number of images, height, and width.\n",
        "* train_images.dtype → Tells the type of data (here: integers from 0 to 255)."
      ],
      "metadata": {
        "id": "B0Uzpicus2QE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv5rBUgXjPBy"
      },
      "source": [
        "**Displaying the fourth digit**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WwLoz5WnjPBy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "collapsed": true,
        "outputId": "c528dd42-b74c-4369-e8ba-91e0662fbd7e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "digit = train_images[4]\n",
        "plt.imshow(digit, cmap=plt.cm.binary)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YCqB65fLjPBy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06a27808-30d3-4916-e96f-448da7e7a695"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.uint8(9)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "train_labels[4]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First code shows you what one digit (a handwritten number) looks like from the training data.**\n",
        "\n",
        "* import matplotlib.pyplot as plt\n",
        "Imports a library for showing images and plots.\n",
        "\n",
        "* digit = train_images[4]\n",
        "Picks image number 5 (index 4) from the training set.\n",
        "\n",
        "* plt.imshow(digit, cmap=plt.cm.binary)\n",
        "Shows the image in black and white (binary colors).\n",
        "\n",
        "* plt.show()\n",
        "Displays the image in a pop-up window or notebook output.\n",
        "\n",
        "**Second code. This gives you the label (correct digit) for the 5th image in the training set (index 4).**\n",
        "\n",
        "* train_images → These are the pictures of handwritten digits (28x28 pixels).\n",
        "* train_labels → These are the correct answers (what number is in each picture).\n"
      ],
      "metadata": {
        "id": "UqN6-kNtt9ZZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiRfjEAejPBy"
      },
      "source": [
        "### Manipulating tensors in NumPy\n",
        "Tensor slicing - selecting elements in a tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "GZN1aLg-jPBy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4723ab9a-78f6-4c42-b7f1-4910e5786f77"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "my_slice = train_images[10:100]\n",
        "my_slice.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "train_images[10:100]\n",
        "* Takes a slice of the training images — from image 10 to 99 (total of 90 images).\n",
        "* This creates a new set with 90 images from the training data.\n",
        "\n",
        "my_slice.shape\n",
        "* Shows the shape (size) of the sliced data."
      ],
      "metadata": {
        "id": "1ClGpUyXv8mg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "93hEfunLjPBy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94607df4-9184-497b-bac7-cbe08793f510"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "my_slice = train_images[10:100, :, :]\n",
        "my_slice.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 10:100 → Selects images from index 10 to 99 (90 images)\n",
        "* : → Keeps all rows (28 pixels high)\n",
        "* : → Keeps all columns (28 pixels wide)\n",
        "\n",
        "This is the same as before — you are taking 90 full images, and each image keeps its full pixel data. The slicing is just written more explicitly."
      ],
      "metadata": {
        "id": "SCu_B-IKwbZi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Nmk32TJZjPBy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd2496e9-e6f2-48f8-ebdd-7af38ee1c7da"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "my_slice = train_images[10:100, 0:28, 0:28]\n",
        "my_slice.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code takes 90 images and includes the full 28×28 pixels of each — just like before, but written more clearly with exact pixel ranges."
      ],
      "metadata": {
        "id": "RkjW-lHlwpJH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "eh0ir4NljPBy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a720431-3df1-4a3d-d732-c553ce09bfd4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 14, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "my_slice = train_images[:, 14:, 14:]\n",
        "my_slice.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* : → Selects all images\n",
        "* 14: → From row 14 to 27 (lower half of the image)\n",
        "* 14: → From column 14 to 27 (right half of the image)\n",
        "\n",
        "This slices out the lower right 14×14 area from all 60,000 images in the training set."
      ],
      "metadata": {
        "id": "BtpPWVcnw-1w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "gqqXEpjsjPBy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11e86fa9-7536-4c00-95d6-69ae6632c2f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 14, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "my_slice = train_images[:, 7:-7, 7:-7]\n",
        "my_slice.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* : → Selects all 60,000 images\n",
        "* 7:-7 → Takes rows from index 7 to index 20 (excluding the first and last 7 rows)\n",
        "* 7:-7 → Takes columns from index 7 to index 20 (excluding the first and last 7 columns)\n",
        "\n",
        "This keeps the central 14×14 pixels from each image and removes the outer edges (7 pixels from each side)."
      ],
      "metadata": {
        "id": "YgkE7ubbxUEl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WdFKn9xjPBy"
      },
      "source": [
        "### The notion of data batches\n",
        "Deep learning models don’t look at all the data at once.\n",
        "Instead, they split the data into small parts, called batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "en-nTBZijPBy"
      },
      "outputs": [],
      "source": [
        "batch = train_images[:128]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "FoFNzSVgjPBy"
      },
      "outputs": [],
      "source": [
        "batch = train_images[128:256]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "GKQwo_6xjPBy"
      },
      "outputs": [],
      "source": [
        "n = 3\n",
        "batch = train_images[128 * n:128 * (n + 1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "batch = train_images[:128]\n",
        "* Selects the first 128 images from the training set\n",
        "* You create a batch of 128 images, often used to feed into the model during training.\n",
        "\n",
        "batch = train_images[128:256]\n",
        "* You are now selecting the next 128 images from the training set — this is how batches are often loaded during training.\n",
        "\n",
        "n = 3\n",
        "batch = train_images[128 * n:128 * (n + 1)]\n",
        "* n = 3 → You want the 4th batch (since counting starts at 0)\n",
        "* 128 * 3 = 384\n",
        "* 128 * (3 + 1) = 512\n",
        "\n",
        "The 128 images from index 384 to 511"
      ],
      "metadata": {
        "id": "p1IfQ8xxyJ2N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsWJ7P92jPBz"
      },
      "source": [
        "## The gears of neural networks: tensor operations - F2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**General**"
      ],
      "metadata": {
        "id": "hStLgp1C4upY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What Is a Layer?**\n",
        "* In deep learning, a layer is just a function.\n",
        "It takes input, does some math, and gives output."
      ],
      "metadata": {
        "id": "-k-UX_x62FbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.layers.Dense(512, activation=\"relu\")"
      ],
      "metadata": {
        "id": "1c17mN9J2a6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We want 512 neurons\n",
        "* Each will use the ReLU activation function\n",
        "* It’s a fully connected (Dense) layer"
      ],
      "metadata": {
        "id": "2qOeIw_C2erN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How Does It Work?**\n",
        "\n",
        "It does this calculation:"
      ],
      "metadata": {
        "id": "JTUU_bdv27w2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = relu(dot(input, W) + b)"
      ],
      "metadata": {
        "id": "BToYyalD29rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* dot(input, W) → Multiply the input by weights (W)\n",
        "* +b → Add a bias term (b)\n",
        "* relu(...) → Apply the activation function (ReLU = only keep positive values)\n",
        "\n",
        "A layer is a function that takes input, does some math with weights and bias,\n",
        "and applies an activation to create the output."
      ],
      "metadata": {
        "id": "E1jVUxfm2_fA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukkNjIvHjPBz"
      },
      "source": [
        "### Element-wise operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3G0UxDqNjPBz"
      },
      "outputs": [],
      "source": [
        "def naive_relu(x):\n",
        "    assert len(x.shape) == 2\n",
        "    x = x.copy()\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            x[i, j] = max(x[i, j], 0)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xggghZ4CjPBz"
      },
      "outputs": [],
      "source": [
        "def naive_add(x, y):\n",
        "    assert len(x.shape) == 2\n",
        "    assert x.shape == y.shape\n",
        "    x = x.copy()\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            x[i, j] += y[i, j]\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8p0AKFInjPBz"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "x = np.random.random((20, 100))\n",
        "y = np.random.random((20, 100))\n",
        "\n",
        "t0 = time.time()\n",
        "for _ in range(1000):\n",
        "    z = x + y\n",
        "    z = np.maximum(z, 0.)\n",
        "print(\"Took: {0:.2f} s\".format(time.time() - t0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8Dy2bZljPBz"
      },
      "outputs": [],
      "source": [
        "t0 = time.time()\n",
        "for _ in range(1000):\n",
        "    z = naive_add(x, y)\n",
        "    z = naive_relu(z)\n",
        "print(\"Took: {0:.2f} s\".format(time.time() - t0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezVSeV7ljPBz"
      },
      "source": [
        "### Broadcasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaUlxhYMjPBz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "X = np.random.random((32, 10))\n",
        "y = np.random.random((10,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vv5iSNTejPBz"
      },
      "outputs": [],
      "source": [
        "y = np.expand_dims(y, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upuZSAVdjPBz"
      },
      "outputs": [],
      "source": [
        "Y = np.concatenate([y] * 32, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AECCehwSjPBz"
      },
      "outputs": [],
      "source": [
        "def naive_add_matrix_and_vector(x, y):\n",
        "    assert len(x.shape) == 2\n",
        "    assert len(y.shape) == 1\n",
        "    assert x.shape[1] == y.shape[0]\n",
        "    x = x.copy()\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            x[i, j] += y[j]\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMTHSC-hjPBz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "x = np.random.random((64, 3, 32, 10))\n",
        "y = np.random.random((32, 10))\n",
        "z = np.maximum(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuiN_feRjPBz"
      },
      "source": [
        "### Tensor product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6_MrDFqjPBz"
      },
      "outputs": [],
      "source": [
        "x = np.random.random((32,))\n",
        "y = np.random.random((32,))\n",
        "z = np.dot(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rqo6NujLjPBz"
      },
      "outputs": [],
      "source": [
        "def naive_vector_dot(x, y):\n",
        "    assert len(x.shape) == 1\n",
        "    assert len(y.shape) == 1\n",
        "    assert x.shape[0] == y.shape[0]\n",
        "    z = 0.\n",
        "    for i in range(x.shape[0]):\n",
        "        z += x[i] * y[i]\n",
        "    return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhKnA6WDjPB0"
      },
      "outputs": [],
      "source": [
        "def naive_matrix_vector_dot(x, y):\n",
        "    assert len(x.shape) == 2\n",
        "    assert len(y.shape) == 1\n",
        "    assert x.shape[1] == y.shape[0]\n",
        "    z = np.zeros(x.shape[0])\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            z[i] += x[i, j] * y[j]\n",
        "    return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82WVCARNjPB0"
      },
      "outputs": [],
      "source": [
        "def naive_matrix_vector_dot(x, y):\n",
        "    z = np.zeros(x.shape[0])\n",
        "    for i in range(x.shape[0]):\n",
        "        z[i] = naive_vector_dot(x[i, :], y)\n",
        "    return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juR5LS6BjPB0"
      },
      "outputs": [],
      "source": [
        "def naive_matrix_dot(x, y):\n",
        "    assert len(x.shape) == 2\n",
        "    assert len(y.shape) == 2\n",
        "    assert x.shape[1] == y.shape[0]\n",
        "    z = np.zeros((x.shape[0], y.shape[1]))\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(y.shape[1]):\n",
        "            row_x = x[i, :]\n",
        "            column_y = y[:, j]\n",
        "            z[i, j] = naive_vector_dot(row_x, column_y)\n",
        "    return z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP5-Zo0DjPB0"
      },
      "source": [
        "### Tensor reshaping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PwpzHerjPB0"
      },
      "outputs": [],
      "source": [
        "train_images = train_images.reshape((60000, 28 * 28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d7NIWi0jPB0"
      },
      "outputs": [],
      "source": [
        "x = np.array([[0., 1.],\n",
        "             [2., 3.],\n",
        "             [4., 5.]])\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlcPH90ajPB0"
      },
      "outputs": [],
      "source": [
        "x = x.reshape((6, 1))\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAdcMlZNjPB0"
      },
      "outputs": [],
      "source": [
        "x = np.zeros((300, 20))\n",
        "x = np.transpose(x)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlQzNUuxjPB0"
      },
      "source": [
        "## The engine of neural networks: gradient-based optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VQZGzihjPB0"
      },
      "source": [
        "### Chaining derivatives: The Backpropagation algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyEUtB9NjPB0"
      },
      "source": [
        "#### The chain rule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "446sevBSjPB0"
      },
      "source": [
        "#### Automatic differentiation with computation graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fot9EXpkjPB0"
      },
      "source": [
        "#### The gradient tape in TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHkdyTUhjPB0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "x = tf.Variable(0.)\n",
        "with tf.GradientTape() as tape:\n",
        "    y = 2 * x + 3\n",
        "grad_of_y_wrt_x = tape.gradient(y, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6HVnApMjPB0"
      },
      "outputs": [],
      "source": [
        "x = tf.Variable(tf.random.uniform((2, 2)))\n",
        "with tf.GradientTape() as tape:\n",
        "    y = 2 * x + 3\n",
        "grad_of_y_wrt_x = tape.gradient(y, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BV-4J24MjPB0"
      },
      "outputs": [],
      "source": [
        "W = tf.Variable(tf.random.uniform((2, 2)))\n",
        "b = tf.Variable(tf.zeros((2,)))\n",
        "x = tf.random.uniform((2, 2))\n",
        "with tf.GradientTape() as tape:\n",
        "    y = tf.matmul(x, W) + b\n",
        "grad_of_y_wrt_W_and_b = tape.gradient(y, [W, b])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olt4rrrCjPB1"
      },
      "source": [
        "## Looking back at our first example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSeBB_3UjPB1"
      },
      "outputs": [],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype(\"float32\") / 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gr2i3P7njPB1"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Dense(512, activation=\"relu\"),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXVTP4vbjPB1"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiNtLORDjPB1"
      },
      "outputs": [],
      "source": [
        "model.fit(train_images, train_labels, epochs=5, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6Ipdfw9jPB1"
      },
      "source": [
        "### Reimplementing our first example from scratch in TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_y-zn3TjPB1"
      },
      "source": [
        "#### A simple Dense class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhOml76CjPB1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class NaiveDense:\n",
        "    def __init__(self, input_size, output_size, activation):\n",
        "        self.activation = activation\n",
        "\n",
        "        w_shape = (input_size, output_size)\n",
        "        w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)\n",
        "        self.W = tf.Variable(w_initial_value)\n",
        "\n",
        "        b_shape = (output_size,)\n",
        "        b_initial_value = tf.zeros(b_shape)\n",
        "        self.b = tf.Variable(b_initial_value)\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        return self.activation(tf.matmul(inputs, self.W) + self.b)\n",
        "\n",
        "    @property\n",
        "    def weights(self):\n",
        "        return [self.W, self.b]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JjjLi2RjPB1"
      },
      "source": [
        "#### A simple Sequential class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fQ6lnZ6jPB1"
      },
      "outputs": [],
      "source": [
        "class NaiveSequential:\n",
        "    def __init__(self, layers):\n",
        "        self.layers = layers\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        x = inputs\n",
        "        for layer in self.layers:\n",
        "           x = layer(x)\n",
        "        return x\n",
        "\n",
        "    @property\n",
        "    def weights(self):\n",
        "       weights = []\n",
        "       for layer in self.layers:\n",
        "           weights += layer.weights\n",
        "       return weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PF0Fa3FvjPB1"
      },
      "outputs": [],
      "source": [
        "model = NaiveSequential([\n",
        "    NaiveDense(input_size=28 * 28, output_size=512, activation=tf.nn.relu),\n",
        "    NaiveDense(input_size=512, output_size=10, activation=tf.nn.softmax)\n",
        "])\n",
        "assert len(model.weights) == 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw8SSANTjPB1"
      },
      "source": [
        "#### A batch generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZArD-w9NjPB1"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class BatchGenerator:\n",
        "    def __init__(self, images, labels, batch_size=128):\n",
        "        assert len(images) == len(labels)\n",
        "        self.index = 0\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.num_batches = math.ceil(len(images) / batch_size)\n",
        "\n",
        "    def next(self):\n",
        "        images = self.images[self.index : self.index + self.batch_size]\n",
        "        labels = self.labels[self.index : self.index + self.batch_size]\n",
        "        self.index += self.batch_size\n",
        "        return images, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpNX6VQLjPB1"
      },
      "source": [
        "### Running one training step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBqzZM01jPB1"
      },
      "outputs": [],
      "source": [
        "def one_training_step(model, images_batch, labels_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(images_batch)\n",
        "        per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
        "            labels_batch, predictions)\n",
        "        average_loss = tf.reduce_mean(per_sample_losses)\n",
        "    gradients = tape.gradient(average_loss, model.weights)\n",
        "    update_weights(gradients, model.weights)\n",
        "    return average_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taOBSkuVjPB1"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "\n",
        "def update_weights(gradients, weights):\n",
        "    for g, w in zip(gradients, weights):\n",
        "        w.assign_sub(g * learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01oHtSkrjPB1"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import optimizers\n",
        "\n",
        "optimizer = optimizers.SGD(learning_rate=1e-3)\n",
        "\n",
        "def update_weights(gradients, weights):\n",
        "    optimizer.apply_gradients(zip(gradients, weights))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9rivXQMjPB1"
      },
      "source": [
        "### The full training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0j74NA9RjPB1"
      },
      "outputs": [],
      "source": [
        "def fit(model, images, labels, epochs, batch_size=128):\n",
        "    for epoch_counter in range(epochs):\n",
        "        print(f\"Epoch {epoch_counter}\")\n",
        "        batch_generator = BatchGenerator(images, labels)\n",
        "        for batch_counter in range(batch_generator.num_batches):\n",
        "            images_batch, labels_batch = batch_generator.next()\n",
        "            loss = one_training_step(model, images_batch, labels_batch)\n",
        "            if batch_counter % 100 == 0:\n",
        "                print(f\"loss at batch {batch_counter}: {loss:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3S1AWicjPB1"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype(\"float32\") / 255\n",
        "\n",
        "fit(model, train_images, train_labels, epochs=10, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv2auzc2jPB2"
      },
      "source": [
        "### Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0wJub58jPB2"
      },
      "outputs": [],
      "source": [
        "predictions = model(test_images)\n",
        "predictions = predictions.numpy()\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "matches = predicted_labels == test_labels\n",
        "print(f\"accuracy: {matches.mean():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNsvCjpPjPB2"
      },
      "source": [
        "## Summary"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "chapter02_mathematical-building-blocks.i",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}